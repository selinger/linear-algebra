\section{Basis and dimension}

\begin{outcome}
  \begin{enumerate}
  \item Find a basis of a given vector space.
  \item Determine the dimension of a vector space.
  \item Extend a linearly independent set of vectors to a basis.
  \item Shrink a spanning set of vectors to a basis.
  \end{enumerate}
\end{outcome}

\begin{definition}{Basis}{basis-vector-space}
  Let $V$ be a vector space. A set $B$ of vectors is called a
  \textbf{basis}%
  \index{basis}%
  \index{basis!of a vector space}%
  \index{vector space!basis} of $V$ if
  \begin{enumerate}
  \item $B$ is a spanning set for $V$, and
  \item $B$ is linearly independent.
  \end{enumerate}
\end{definition}

\begin{example}{Bases of $\Poly_2$}{basis-p2}
  Consider the vector space $\Poly_2$ of polynomials of degree at most
  2 with coefficients in a field $K$.%
  \index{P2@$\Poly_2$!basis of}
  \begin{itemize}
  \item $\set{1,~ x,~ x^2}$ is a basis of $\Poly_2$.
  \item $\set{x^2,~ (x+1)^2,~ (x+2)^2}$ is a basis of $\Poly_2$.
  \item $\set{1,~ x-1,~ (x-1)^2}$ is a basis of $\Poly_2$.
  \end{itemize}
  Unlike $\R^n$, a vector space like $\Poly_2$ does not
  necessarily have a ``standard'' basis. One basis might be useful for
  one application, and another basis for a different application.
\end{example}

\begin{proof}
  It is easy to verify that each set of vectors is linearly
  independent and spanning. See Examples~\ref{exa:spanning-set},
  {\ref{exa:linear-independence-polynomial}}, and
  {\ref{exa:polynomials-increasing-degree}} for similar calculations.
\end{proof}

\begin{example}{An infinite basis}{basis-p}
  Consider the vector space $\Poly$ of all polynomials with
  coefficients in a field $K$. The following is a basis for $\Poly$%
  \index{P@$\Poly$!basis of}:
  \begin{equation*}
    \set{1,~ x,~ x^2,~ x^3,~ x^4,~ \ldots}.
  \end{equation*}
  Note that this basis is infinite.
\end{example}

\begin{proof}
  The polynomials $1$, $x$, $x^2$, $x^3$, $x^4$, $\ldots$ are linearly
  independent by Proposition~\ref{prop:linear-dependence-redundant}.
  Namely, if they were linearly dependent, then one of the polynomials
  could be written as a linear combination of earlier ones. However,
  this is not possible because a polynomial of degree $n$ cannot be a
  linear combination of polynomials of degree less than $n$.

  To show that the polynomials $1$, $x$, $x^2$, $x^3$, $x^4$, $\ldots$
  are a spanning set, consider an arbitrary element $p(x)$ of $\Poly$.
  Then by definition, $p(x)$ is of the form
  \begin{equation*}
    p(x) = a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0,
  \end{equation*}
  for some $n\geq 0$ and $a_0,\ldots,a_n\in K$. But then $p(x)$ is a
  linear combination of $1,\ldots,x^n$, i.e., it is in the span of
  $\set{1,~ x,~ x^2,~ x^3,~ x^4,~ \ldots}$.
\end{proof}

\begin{example}{Not a basis}{not-a-basis}
  Consider the vector space $\Seq_K$ of infinite sequences. As before,
  let $e^k$ be the sequence whose $k\th$ element is $1$ and that is
  $0$ everywhere else, i.e.,
    \begin{equation*}
    \begin{array}{l}
      e^0 = (1,0,0,0,0,\ldots), \\
      e^1 = (0,1,0,0,0,\ldots), \\
      e^2 = (0,0,1,0,0,\ldots), \\
    \end{array}
  \end{equation*}
  and so on. Then the set
  \begin{equation*}
    \set{e^0,~ e^1,~ e^2,~ \ldots}
  \end{equation*}
  is \textit{not} a basis of $\Seq_K$. Indeed, although we saw in
  Example~\ref{exa:linear-independence-sequences} that the sequences
  $e^0$, $e^1$, $e^2$, $\ldots$ are linearly independent,
  Example~\ref{exa:spans-sequences} shows that they are not spanning.
  Indeed,
  \begin{equation*}
    W = \sspan\set{e^0,~ e^1,~ e^2,~ \ldots}
  \end{equation*}
  is a subspace of $\Seq_K$, consisting exactly of the
  \textbf{finitely supported}%
  \index{finitely supported sequence}%
  \index{sequence!finitely supported} sequences, i.e., those sequences
  that have only finitely many non-zero components. Thus, $\set{e^0,~ e^1,~
    e^2,~ \ldots}$ is a basis of $W$.
\end{example}

The following theorem ensures that every vector space has a basis.  We
will not prove this theorem, because when the spaces are
infinite-dimensional, the proof uses mathematics that is beyond the
scope of this book. The proof uses a reasoning principle called the
\textbf{axiom of choice}%
\index{axiom of choice}%
\index{choice!axiom of}, which allows us to prove the existence of a
basis even in cases where we cannot find an actual concrete example of
a basis.  For example, it is not possible to give a specific example
of a basis for the space $\Seq_K$, even though the following theorem
guarantees that such a basis exists.

\begin{theorem}{Existence of bases}{basis-existence}
  Every vector space has a basis.
\end{theorem}

The Exchange Lemma, which we proved in the context of $\R^n$ in
Section~\ref{sec:basis-and-dimension}, is true in general vector
spaces.

\begin{lemma}{Exchange Lemma}{exchange-lemma-vector-space}
  \index{exchange lemma!in a vector space}%
  Let $V$ be a vector space over a field $K$. Suppose
  $\vect{u}_1,\ldots,\vect{u}_r$ are linearly independent elements
  of $\sspan\set{\vect{v}_1,\ldots,\vect{v}_s}$. Then $r\leq s$.
\end{lemma}

The proof is exactly the same as that of
Lemma~\ref{lem:exchange-lemma}, so we do not repeat it here.  As in
Section~\ref{sec:basis-and-dimension}, an important consequence of the
Exchange Lemma is that any two bases of a vector space have the same
size.

\begin{theorem}{Bases are of the same size}{basis-same-size-vector-space}
  \index{basis!size of}%
  Let $V$ be a vector space over some field $K$, and let $B_1$ and
  $B_2$ be bases of $V$. Then either $B_1$ and $B_2$ are both finite
  and have the same number of elements, or else $B_1$ and $B_2$ are
  both infinite.
\end{theorem}

\begin{proof}
  We first show that $B_1$ and $B_2$ are either both finite or both
  infinite. Assume one of them, say $B_1$, is finite and contains $s$
  vectors. Since $B_1$ is spanning and $B_2$ is linearly independent,
  it follows from the Exchange Lemma that $B_2$ cannot contain more
  than $s$ vectors, and in particular, $B_2$ must be finite.  So the
  sets are either both finite or both infinite. If they are both
  finite, say of size $s$ and $r$, then by the Exchange Lemma, we have
  $s\leq r$ and $r\leq s$, hence $r=s$.
\end{proof}

This allows us to define the dimension of a vector space.

\begin{definition}{Dimension}{dimension-vector-space}
  Let $V$ be a vector space over a field $K$. If $V$ has a basis
  consisting of $n$ vectors, we say that $V$ has \textbf{dimension}%
  \index{dimension!of vector space}%
  \index{vector space!dimension} $n$, and we write $\dim(V)=n$. In
  this case we also say that $V$ is \textbf{finite-dimensional}%
  \index{finite-dimensional space}%
  \index{vector space!finite-dimensional}. If $V$ has an infinite
  basis, we say that $V$ is \textbf{infinite-dimensional}%
  \index{infinite-dimensional space}%
  \index{vector space!infinite-dimensional}, and we write
  $\dim(V) = \infty$.
\end{definition}

Note that the dimension is well-defined by Theorems
{\ref{thm:basis-existence}} and
{\ref{thm:basis-same-size-vector-space}}, since these theorems ensure
that every vector space has a basis (and therefore a dimension), and
that any two bases are of the same size (and therefore a vector space
cannot have more than one dimension).

We now calculate the dimensions of some vector spaces we encountered in
Sections~\ref{sec:definition-vector-spaces} and
{\ref{sec:vector-space-subspaces}}.

\begin{itemize}
\item The space $\R^n$ has dimension $n$.
\item The space $\Poly_2$ has dimension $3$. We found several bases
  for this space in Example~\ref{exa:basis-p2}.
\item The space $\Mat_{m,n}$ has dimension $mn$. A possible basis
  consists of all the matrices that contain a single $1$ and zeros
  everywhere else.
\item The space $\Func_{X,K}$ is infinite-dimensional if $X$ is an
  infinite set. If $X$ is a finite set of $n$ elements, then this
  space is $n$-dimensional. In that case, a basis is given by the set
  of functions whose value is $1$ for one input and $0$ for all other
  inputs.
\item The space $\Seq_K$ is infinite-dimensional. We found an infinite
  linearly independent set in
  Example~\ref{exa:linear-independence-sequences}, showing that the
  space cannot be finite-dimensional.
\item The space $\Poly$ is infinite-dimensional. We found a basis for
  this space in Example~\ref{exa:basis-p}.
\item The subspace of $\Func_{\R,\R}$ consisting of the continuous
  functions is infinite-dimensional. For example, the functions
  $\set{1,x,x^2,x^3,\ldots}$ form an infinite, linearly independent
  set of continuous functions.
\item The subspace of $\Func_{\R,\R}$ consisting of the differentiable
  functions is infinite-dimensional. Again, the set
  $\set{1,x,x^2,x^3,\ldots}$ is an infinite linearly independent set
  in this space.
\end{itemize}

\begin{example}{Space of sequences satisfying a linear recurrence}{subspace-recurrence-dimension}
  In Example~\ref{exa:subspace-recurrence}, we considered the space
  $W$ of sequences of real numbers that satisfy the recurrence%
  \index{recurrence!as a subspace}
  $a_{n+2}=a_n+a_{n+1}$. What is the dimension of this space?
\end{example}

\begin{solution}
  The space is $2$-dimensional. The easiest way to see this is to
  observe that a sequence $a\in W$ is determined by its first two
  elements. We can say that the first two elements of the sequence are
  parameters, and all the other elements are then computed by the
  recurrence relation. Specifically, suppose $a_0=x$ and
  $a_1=y$. Using the recurrence relation to compute the remaining
  elements, we have
  \begin{eqnarray*}
    a &=& (x,~y,~x+y,~x+2y,~2x+3y,~3x+5y,~\ldots) \\
      &=& x(1,0,1,1,2,3,\ldots) + y(0,1,1,2,3,5,\ldots).
  \end{eqnarray*}
  Since this is the general form of the elements of $W$, and since the
  two sequences starting with 1,0 and 0,1 are clearly linearly
  independent, it follows that
  \begin{equation*}
    \set{(1,0,1,1,2,3,\ldots),~ (0,1,1,2,3,5,\ldots)}
  \end{equation*}
  is a basis of $W$.
\end{solution}

\begin{example}{Solution space of a linear differential equation}{dimension-differential-equation}
  In Example~\ref{exa:subspace-differential-equation}, we considered
  the space of solutions of the differential equation $f''=-f$. What
  is the dimension of this space?
\end{example}

\begin{solution}
  From calculus, we know that the general solution of the differential
  equation $f''=-f$ is
  \begin{equation*}
    f(x) = A\sin x + B\cos x,
  \end{equation*}
  where $A,B$ are constants. We also know, from
  Example~\ref{exa:linearly-independent-functions}, that $\sin x$ and
  $\cos x$ are linearly independent. It follows that
  $\set{\sin x, \cos x}$ is a basis for the solution space. The
  solution space is therefore $2$-dimensional.
\end{solution}

We conclude this section by stating two properties of bases that
generalize Theorem~\ref{thm:linearly-independent-subset} and
Lemma~\ref{lem:extend-to-basis}: every linearly independent set can be
extended to a basis by adding 0 or more vectors, and every spanning
set can be reduced to a basis by removing 0 or more vectors.

\begin{proposition}{Extending a linearly independent set to a basis}{basis-from-linear-independent}
  Let $V$ be a vector space, and let $S\subseteq V$ be a linearly
  independent set of vectors. Then $S$ can be extended to a basis of
  $V$, i.e., there exists a basis $B$ of $V$ such that $S\subseteq
  B$.%
  \index{linear independence!extending to basis}%
  \index{basis!by extending linearly independent set}
\end{proposition}

\begin{proposition}{Shrinking a spanning set to a basis}{basis-from-spanning}
  \index{spanning set!shrink to basis}%
  \index{basis!by shrinking spanning set}%
  Let $V$ be a vector space, and let $S\subseteq V$ be a spanning set
  of $V$. Then $S$ can be shrunk to a basis, i.e., there exists a
  basis $B$ of $V$ such that $B\subseteq S$.
\end{proposition}

\begin{example}{Extending a linearly independent set to a basis}{adding-linear-independent-basis}
  Let $S \subseteq \Mat_{2,2}$ be the linearly independent set given by
  \begin{equation*}
    S  = \set{
      \begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix},~
      \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix}
    }
  \end{equation*}
  Enlarge $S$ to a basis of $\Mat_{2,2}$.
\end{example}

\begin{solution}
  We can obtain a basis of $\Mat_{2,2}$ by adding two more linearly
  independent matrices
  \begin{equation*}
    \begin{mymatrix}{rr}
      0 & 0 \\
      1 & 0
    \end{mymatrix}
    \quad\mbox{and}\quad
    \begin{mymatrix}{rr}
      0 & 0 \\
      0 & 1
    \end{mymatrix}.
  \end{equation*}
  The resulting basis is
  \begin{equation*}
    B = \set{
      \begin{mymatrix}{rr}
        1 & 0 \\
        0 & 0
      \end{mymatrix},~
      \begin{mymatrix}{rr}
        0 & 1 \\
        0 & 0
      \end{mymatrix},~
      \begin{mymatrix}{rr}
        0 & 0 \\
        1 & 0
      \end{mymatrix},~
      \begin{mymatrix}{rr}
        0 & 0 \\
        0 & 1
      \end{mymatrix}
    }.
  \end{equation*}
\end{solution}

\begin{example}{Shrinking a spanning set to a basis}{shrink-spanning}
  Consider the spanning set $S \subseteq \Poly_2$ given by
  \begin{equation*}
    S = \set{1,~ x,~ 2x+1,~ x^2+1,~ x^2+2}
  \end{equation*}
  Shrink $S$ to a basis of $\Poly_2$.
\end{example}

\begin{solution}
  We use a version of the casting-out method. We examine each element
  $S$ from left to right and cast out the elements that are linear
  combinations of previous elements. Clearly the first two elements,
  $1$ and $x$, are linearly independent. The next element, $2x+1$, is
  redundant because it is a linear combination of $1$ and $x$. The
  next element $x^2+1$ is linearly independent of $1$ and $x$. The
  final element $x^2+2$ is redundant because it is a linear
  combination of $1$ and $x^2+1$. Therefore, the following subset of
  $S$ is a basis of $\Poly_2$:
  \begin{equation*}
    B = \set{1,~ x,~ x^2+1}.
  \end{equation*}
\end{solution}
