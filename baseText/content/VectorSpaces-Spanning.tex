\section{Linear combinations, span, and linear independence}

\begin{outcome}
  \begin{enumerate}
  \item Determine if a vector is within a given span.
  \item Determine if a set is spanning.
  \item Determine if a set is linearly independent.
  \end{enumerate}
\end{outcome}

In this section, we will again explore concepts introduced earlier in terms of $\R^n$ and extend them to apply to abstract vector spaces.

We can now revisit many of the concepts first introduced in
Chapter~\ref{cha:vectors-rn} in the context of general vector spaces.
We will look at linear combinations, span, and linear independence in
this section, and at subspaces, bases, and dimension in the next
section.

\begin{definition}{Linear combination}{linear-combination-vector-space}
  Let $V$ be a vector space over a field $K$. Let
  $\vect{u}_1,\ldots,\vect{u}_n\in V$. A vector
  $\vect{v}\in V$ is called a \textbf{linear combination}%
  \index{linear combination!in a vector space}%
  \index{linear combination!of vectors} of
  $\vect{u}_1,\ldots,\vect{u}_n$ if there exist scalars
  $a_1,\ldots,a_n\in K$ such that
  \begin{equation*}
    \vect{v} = a_1 \vect{u}_1 + \ldots + a_n \vect{u}_n.
  \end{equation*}
\end{definition}

\begin{example}{Linear combination of matrices}{linear-combination-matrix}
  Write the matrix $A=\begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}$
  as a linear combination%
  \index{linear combination!of matrices} of
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 0 \\ 0 & 1 \end{mymatrix},\quad
    \begin{mymatrix}{rr} 1 & 0 \\ 0 & -1 \end{mymatrix},\quad
    \begin{mymatrix}{rr} 0 & 1 \\ 1 & 0 \end{mymatrix},\quad\mbox{and}\quad
    \begin{mymatrix}{rr} 0 & -1 \\ 1 & 0 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  We must find coefficients $a,b,c,d$ such that
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}
    ~=~ a \begin{mymatrix}{rr} 1 & 0 \\ 0 & 1 \end{mymatrix}
    + b \begin{mymatrix}{rr} 1 & 0 \\ 0 & -1 \end{mymatrix}
    + c \begin{mymatrix}{rr} 0 & 1 \\ 1 & 0 \end{mymatrix}
    + d \begin{mymatrix}{rr} 0 & -1 \\ 1 & 0 \end{mymatrix},
  \end{equation*}
  or equivalently,
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}
    ~=~ \begin{mymatrix}{cc} a+b & c-d \\ c+d & a-b \end{mymatrix}.
  \end{equation*}
  This yields a system of four equations in four variables:
  \begin{equation*}
    \begin{array}{r@{~~}c@{~}r}
      a+b &=& 1, \\
      c+d &=& -1, \\
      c-d &=& 3, \\
      a-b &=& 2.
    \end{array}
  \end{equation*}
  We can easily solve the system of equations to find the unique
  solution $a=\frac{3}{2}$, $b=-\frac{1}{2}$, $c=1$, $d=-2$.
  Therefore
  \begin{equation*}
    \begin{mymatrix}{rr} 1 & 3 \\ -1 & 2 \end{mymatrix}
    ~=~ \frac{3}{2} \begin{mymatrix}{rr} 1 & 0 \\ 0 & 1 \end{mymatrix}
    - \frac{1}{2} \begin{mymatrix}{rr} 1 & 0 \\ 0 & -1 \end{mymatrix}
    + 1 \begin{mymatrix}{rr} 0 & 1 \\ 1 & 0 \end{mymatrix}
    - 2 \begin{mymatrix}{rr} 0 & -1 \\ 1 & 0 \end{mymatrix}.
  \end{equation*}
\end{solution}

\begin{example}{Linear combination of polynomials}{linear-combination-polynomials}
  Write the polynomial $p(x) = 7x^2 + 4x - 3$ as a linear combination%
  \index{linear combination!of polynomials} of
  \begin{equation*}
    q_1(x) = x^2,\quad
    q_2(x) = (x+1)^2,\quad\mbox{and}\quad
    q_3(x) = (x+2)^2.
  \end{equation*}
\end{example}

\begin{solution}
  Note that $q_2(x) = (x+1)^2 = x^2 + 2x + 1$ and
  $q_3(x) = (x+2)^2 = x^2 + 4x + 4$. We must find coefficients $a,b,c$
  such that $p(x) = aq_1(x) + bq_2(x) + cq_3(x)$, or equivalently,
  \begin{equation*}
    7x^2 + 4x - 3 ~=~ ax^2 ~+~ b(x^2 + 2x + 1) ~+~ c(x^2 + 4x + 4).
  \end{equation*}
  Collecting equal powers of $x$, we can rewrite this as
  \begin{equation*}
    7x^2 + 4x - 3 ~=~ (a+b+c)x^2 ~+~ (2b+4c)x ~+~ (b+4c).
  \end{equation*}
  Since two polynomials are equal if and only if each corresponding
  coefficient is equal, this yields a system of three equations in
  three variables
  \begin{equation*}
    \begin{array}{r@{~~}c@{~}r}
      a+b+c &=& 7, \\
      2b+4c &=& 4, \\
      b+4c &=& -3.
    \end{array}
  \end{equation*}
  We can easily solve this system of equations and find that the
  unique solution is $a=\frac{5}{2}$, $b=7$,
  $c=-\frac{5}{2}$. Therefore
  \begin{equation*}
    p(x) ~=~ \frac{5}{2}\,q_1(x) ~+~ 7\,q_2(x) ~-~ \frac{5}{2}\,q_3(x).
  \end{equation*}
\end{solution}

As in Chapter~\ref{cha:vectors-rn}, the span of a set of vectors is
defined as the set of all of its linear combinations. We generalize
the concept of span to consider spans of arbitrary (possibly finite,
possibly infinite) sets of vectors.

\begin{definition}{Span of a set of vectors}{vector-space-span}
  Let $V$ be a vector space over some field $K$, and let $S$ be a set
  of vectors (i.e., a subset of $V$). The \textbf{span}%
  \index{span}%
  \index{vector!span}%
  \index{vector space!span} of $S$ is the set of all linear
  combinations of elements of $S$. In symbols, we have
  \begin{equation*}
    \sspan S
    ~=~ \set{a_1\vect{u}_1+\ldots+a_k\vect{u}_k \mid
      \mbox{
        $\vect{u}_1,\ldots,\vect{u}_k\in S$
        and
        $a_1,\ldots,a_k\in K$
      }}.
  \end{equation*}
\end{definition}

It is important not to misunderstand this definition.  Even when the
set $S$ is infinite, each {\em individual} element
$\vect{v}\in\sspan S$ is a linear combination of only {\em finitely
  many} elements $\vect{u}_1,\ldots,\vect{u}_k$ of $S$.
The definition does not talk about infinite linear combinations
\begin{equation*}
  a_1\vect{u}_1 + a_2\vect{u}_2 + a_3\vect{u}_3 + \ldots
\end{equation*}
Indeed, such infinite sums do not typically exist.  However, different
elements $\vect{v},\vect{w}\in\sspan S$ can be linear combinations of
a different (finite) number of vectors of $S$. For example, it is
possible that $\vect{v}$ is a linear combination of 10 elements of
$S$, and $\vect{w}$ is a linear combination of 100 elements of $S$.

\begin{example}{Spans of sequences}{spans-sequences}
  Consider the vector space $\Seq_K$ of infinite sequences. For every
  $k\in\N$, let $e^k$ be the sequence whose $k\th$ element is $1$ and
  that is $0$ everywhere else, i.e.,
  \begin{equation*}
    \begin{array}{l}
      e^0 = (1,0,0,0,0,\ldots), \\
      e^1 = (0,1,0,0,0,\ldots), \\
      e^2 = (0,0,1,0,0,\ldots), \\
    \end{array}
  \end{equation*}
  and so on.
  Let $S=\set{e^k \mid k\in\N}$. Which of the following sequences are in
  $\sspan S$?
  \begin{enumialphparenastyle}
    \begin{enumerate}
    \item $f = (1,1,1,0,0,0,0,0,\ldots)$ (followed by infinitely many zeros),
    \item $g = (1,2,0,5,0,0,0,0,\ldots)$ (followed by infinitely many zeros),
    \item $h = (1,1,1,1,1,1,1,1,\ldots)$ (followed by infinitely many ones),
    \item $k = (1,0,1,0,1,0,1,0,\ldots)$ (forever alternating between $1$ and $0$).
    \end{enumerate}
  \end{enumialphparenastyle}
\end{example}

\begin{solution}
  \begin{enumialphparenastyle}
    \begin{enumerate}
    \item We have $f\in\sspan S$, because $f = e^0 + e^1 + e^2$.
    \item We have $g\in\sspan S$, because $g = 1e^0 + 2e^1 + 5e^3$.
    \item The sequence $h$ is not in $\sspan S$, because each element
      of $\sspan S$ is, by definition, a linear combinations of {\em
        finitely many} elements of $S$. No linear combinations of
      finitely many $e^k$ can end in infinitely many ones. Note that
      we are not permitted to write an infinite sum such as
      $e^0+e^1+e^2+\ldots$. Such infinite sums are not defined in
      vector spaces.
    \item The sequence $k$ is not in $\sspan S$, for the same reason.
      We would need to add infinitely many sequences of the form $e^k$
      to get a sequence that contains infinitely many non-zero
      elements. However, this is not permitted by the definition of
      span.
    \end{enumerate}
  \end{enumialphparenastyle}
  \vspace{-4ex}
\end{solution}

\begin{example}{Span of polynomials}{span-of-polynomials}
  Let $p(x)=7x^2+4x-3$. Is $p(x)\in\sspan\set{x^2,~ (x+1)^2,~ (x+2)^2}$?
\end{example}

\begin{solution}
  The answer is yes, because we found in
  Example~\ref{exa:linear-combination-polynomials} that
  $p(x) = \frac{5}{2}\,x^2 ~+~ 7\,(x+1)^2 ~-~ \frac{5}{2}\,(x+2)^2$.
\end{solution}

We say that a set of vectors $S$ is a \textbf{spanning set}%
\index{spanning set}%
\index{vector space!spanning set} for $V$ if $V = \sspan S$.

\begin{example}{Spanning set}{spanning-set}
  Let $S = \set{x^2,~ (x+1)^2,~ (x+2)^2}$. Show that $S$ is a
  spanning set for $\Poly_2$, the vector space of all polynomials of
  degree at most $2$.
\end{example}

\begin{solution}
  This is analogous to Example~\ref{exa:linear-combination-polynomials}.
  Consider an arbitrary element $p(x) = p_2x^2 + p_1x + p_0$ of
  $\Poly_2$. We must show that $p(x)\in\sspan S$, i.e., that there
  exists $a,b,c\in K$ such that
  \begin{equation*}
    p(x) = ax^2 + b(x+1)^2 + c(x+2)^2.
  \end{equation*}
  We can equivalently rewrite this equation as
  \begin{equation*}
    p_2x^2 + p_1x + p_0 ~=~ (a+b+c)x^2 ~+~ (2b+4c)x ~+~ (b+4c),
  \end{equation*}
  which yields the system of equations
  \begin{equation*}
    \begin{array}{r@{~~}c@{~}r}
      a+b+c &=& p_2 \\
      2b+4c &=& p_1 \\
      b+4c &=& p_0
    \end{array}
    \quad\roweq\quad
    \begin{mymatrix}{ccc|c}
      1 & 1 & 1 & p_2 \\
      0 & 2 & 4 & p_1 \\
      0 & 1 & 4 & p_0 \\
    \end{mymatrix}
    \quad\roweq\quad
    \begin{mymatrix}{ccc|c}
      1 & 0 & 0 & p_2-\frac{3}{4}p_1+\frac{1}{2}p_0 \\
      0 & 1 & 0 & p_1-p_0 \\
      0 & 0 & 1 & \frac{1}{2}p_0-\frac{1}{4}p_1 \\
    \end{mymatrix}.
  \end{equation*}
  Since the system has rank 3, it has a solution. Therefore,
  $p(x)\in\sspan S$. Since $p(x)$ was an arbitrary element of
  $\Poly_2$, it follows that $S$ is a spanning set for $\Poly_2$.
\end{solution}

To define the concept of linear independence in a general vector
space, it will be convenient to base our definition on the
``alternative'' characterization of
Theorem~\ref{thm:characterization-linear-independence}. Here too, we
generalize the definition to an arbitrary (finite or infinite) set of
vectors.

\begin{definition}{Linear independence}{linear-independence-vector-space}
  Let $V$ be a vector space over some field $K$. A finite set of
  vectors $\set{\vect{u}_1,\ldots,\vect{u}_k}$ is called
  \textbf{linearly independent}%
  \index{linear independence!in a vector space}%
  \index{vector!linearly independent!in a vector space}%
  \index{vector space!linear independence}
  if the equation
  \begin{equation*}
    a_1\,\vect{u}_1 + \ldots + a_k\,\vect{u}_k = \vect{0}
  \end{equation*}
  has only the trivial solution $a_1,\ldots,a_k=0$. An infinite set
  $S$ of vectors is called linearly independent if every finite subset
  of $S$ is linearly independent. A set of vectors is called
  \textbf{linearly dependent}%
  \index{linear dependence}%
  \index{vector!linearly dependent} if it is not linearly independent.
\end{definition}

\begin{example}{Linearly independent polynomials}{linear-independence-polynomial}
  Determine whether the polynomials $x^2$, $x^2 + 2x - 1$, and
  $2x^2 - x + 3$ are linearly independent.
\end{example}

\begin{solution}
  According to the definition of linear independence, we must solve
  the equation
  \begin{equation*}
    ax^2 + b(x^2 + 2x - 1) + c(2x^2 - x + 3) ~=~ 0.
  \end{equation*}
  If there is a non-trivial solution, the polynomials are linearly
  dependent. If there is only the trivial solution, they are linearly
  independent. We first rearrange the left-hand side to collect equal
  powers of $x$:
  \begin{equation*}
    (a + b + 2c)x^2 + (2b - c)x + (3c - b) ~=~ 0.
  \end{equation*}
  This turns into a system of 3 equations in 3 variables:
  \begin{equation*}
    \begin{array}{rcl}
      a + b + 2c &=& 0 \\
      2b - c &=& 0 \\
      3c - b &=& 0
    \end{array}
    \quad\roweq\quad
    \begin{mymatrix}{rrr|r}
      1 &  1 &  2 & 0 \\
      0 &  2 & -1 & 0 \\
      0 & -1 &  3 & 0
    \end{mymatrix}
    \quad\roweq\quad
    \begin{mymatrix}{rrr|r}
      1 &  0 &  0 & 0 \\
      0 &  1 &  0 & 0 \\
      0 &  0 &  1 & 0
    \end{mymatrix}.
  \end{equation*}
  Since the system has rank 3, there are no free variables. The only
  solution is $a=b=c=0$, and the polynomials are linearly
  independent.
\end{solution}

\begin{example}{Linearly independent sequences}{linear-independence-sequences}
  Let $K$ be a field, and consider again the sequences from
  Example~\ref{exa:spans-sequences},
    \begin{equation*}
    \begin{array}{l}
      e^0 = (1,0,0,0,0,\ldots), \\
      e^1 = (0,1,0,0,0,\ldots), \\
      e^2 = (0,0,1,0,0,\ldots), \\
    \end{array}
  \end{equation*}
  and so on. Let $S=\set{e^0, e^1, e^2,\ldots}$. This is an infinite
  subset of $\Seq_K$. Show that $S$ is linearly independent.
\end{example}

\begin{solution}
  Since $S$ is an infinite set, we have to show that every finite
  subset of $S$ is linearly independent. So consider a finite subset
  \begin{equation*}
    \set{e^{k_1}, e^{k_2}, \ldots, e^{k_n}} \subseteq S
  \end{equation*}
  and assume that
  \begin{equation}\label{eqn:linear-independence-sequences}
    a_1e^{k_1} + a_2e^{k_2} + \ldots + a_ne^{k_n} = 0.
  \end{equation}
  We have to show that $a_1,\ldots,a_n=0$. Consider some index
  $i\in\set{1,\ldots,n}$. Then the $k_i\th$ element of
  $a_1e^{k_1} + \ldots + a_ne^{k_n}$ is equal to $a_i$ by the
  left-hand side of {\eqref{eqn:linear-independence-sequences}}, but
  it is also equal to $0$ by the right-hand side of
  {\eqref{eqn:linear-independence-sequences}}. It follows that $a_i=0$
  for all $i\in\set{1,\ldots,n}$, and therefore
  $\set{e^{k_1}, e^{k_2}, \ldots, e^{k_n}}$ is linearly
  independent. Since $\set{e^{k_1}, e^{k_2}, \ldots, e^{k_n}}$ was an
  arbitrary finite subset of $S$, it follows, by definition, that $S$
  is linearly independent.
\end{solution}

\begin{example}{Linearly dependent matrices}{linearly-dependent-matrices}
  Determine whether the following elements of $\Mat_{2,2}$ are
  linearly independent:
  \begin{equation*}
    M_1 = \begin{mymatrix}{rr} -1 & 0 \\ 1 & -1 \end{mymatrix},\quad
    M_2 = \begin{mymatrix}{rr}  1 & 1 \\ 1 &  2 \end{mymatrix},\quad
    M_3 = \begin{mymatrix}{rr}  1 & 3 \\ 5 &  4 \end{mymatrix}.
  \end{equation*}
\end{example}

\begin{solution}
  To determine whether $\set{M_1,M_2,M_3}$ is linearly independent, we
  look for solutions to
  \begin{equation*}
    aM_1 + bM_2 + cM_3 = 0.
  \end{equation*}
  Notice that this equation has non-trivial solutions, for example
  $a=2$, $b=3$ and $c=-1$. Therefore the matrices are linearly
  dependent.
\end{solution}

\begin{example}{Linearly independent functions}{linearly-independent-functions}
  In the vector space $\Func_{\R,\R}$ of real-valued functions on the
  real numbers, show that the functions $f(x)=\sin x$ and $g(x)=\cos
  x$ are linearly independent.
\end{example}

\begin{solution}
  Assume $A\sin x+B\cos x=0$. Note that this is an equality of
  functions, which means that it is true for all $x$. In particular,
  substituting $x=0$ into the equation, and using the fact that
  $\sin 0=0$ and $\cos 0=1$, we have
  \begin{equation*}
    0 = A\sin 0 + B\cos 0 = A\cdot 0 + B\cdot 1 = B,
  \end{equation*}
  and therefore $B=0$. On the other hand, substituting
  $x=\frac{\pi}{2}$ into the equation, and using the fact that
  $\sin\frac{\pi}{2} = 1$ and $\cos\frac{\pi}{2}=0$, we have
  \begin{equation*}
    0 = A\sin\frac{\pi}{2} + B\cos\frac{\pi}{2} = A\cdot 1 + B\cdot 0
    = A,
  \end{equation*}
  and therefore $A=0$. Therefore, the equation $A\sin x+B\cos x=0$
  only has the trivial solution $A=B=0$, and it follows that $\sin x$
  and $\cos x$ are linearly independent.
\end{solution}

The properties of linear independence that were discussed in
Chapter~\ref{cha:vectors-rn} remain true in the general setting of
vector spaces. For example, the first two parts of
Proposition~\ref{prop:properties-linear-independence} apply without change.
(The third part specifically mentions $\R^n$, but can be generalized
to any vector space of dimension $n$). We also have the usual
characterization of linear dependence in terms of redundant vectors:

\begin{proposition}{Linear dependence and redundant vectors}{linear-dependence-redundant}
  Let $V$ be a vector space, and let $\vect{u}_1,\vect{u}_2,\ldots$ be
  a (finite or infinite) sequence of vectors in $V$. If
  $\vect{u}_1,\vect{u}_2,\ldots$ are linearly dependent, then at least
  one of the vectors can be written as a linear combination of earlier
  vectors in the sequence:
  \begin{equation*}
    \vect{u}_j = a_1\,\vect{u}_1 + a_2\,\vect{u}_2 + \ldots + a_{j-1}\,\vect{u}_{j-1},
  \end{equation*}
  for some $j$.
\end{proposition}

\begin{proof}
  Suppose that the vectors are linearly dependent. Then the equation
  $b_1\vect{u}_1+\ldots+b_k\vect{u}_k=\vect{0}$ has a non-trivial solution
  for some $k$. In other words, there exist scalars $b_1,\ldots,b_k$,
  not all equal to zero, such that
  $b_1\vect{u}_1+\ldots+b_k\vect{u}_k=\vect{0}$. Let $j$ be the largest index
  such that $b_j\neq 0$. Then
  $b_1\vect{u}_1+\ldots+b_j\vect{u}_j=\vect{0}$. Dividing by $b_j$ and
  solving for $\vect{u}_j$, we have
  $\vect{u}_j = -\frac{b_1}{b_j}\vect{u}_1 - \ldots -
  \frac{b_{j-1}}{b_j}\vect{u}_{j-1}$, so $\vect{u}_j$ can be written
  as a linear combination of earlier vectors as claimed.
\end{proof}

\begin{example}{Polynomials of increasing degree}{polynomials-increasing-degree}
  Consider a sequence of non-zero polynomials $p_1(x), \ldots, p_k(x)$
  of increasing degree, i.e., such that the degree of each $p_i(x)$ is
  strictly larger than that of $p_{i-1}(x)$. Show that
  $p_1(x),\ldots,p_k(x)$ are linearly independent in the vector space
  $\Poly$.
\end{example}

\begin{solution}
  A polynomial of degree $n$ cannot be a linear combination of
  polynomials of degree less than $n$. Therefore, none of the
  polynomials $p_1(x), \ldots, p_k(x)$ can be written as a linear
  combination of earlier polynomials. By
  Proposition~\ref{prop:linear-dependence-redundant}, $p_1(x), \ldots,
  p_k(x)$ are linearly independent.
\end{solution}

Theorems~\ref{thm:unique-linear-combination} and
{\ref{thm:linearly-independent-subset}} also remain true in the
setting of general vector spaces. The original proofs can be used
without change. Thus, if $\vect{u}_1,\ldots,\vect{u}_k$ are linearly
independent, then every vector
$\vect{v}\in\sspan\set{\vect{u}_1,\ldots,\vect{u}_k}$ can be uniquely
written as a linear combination of
$\vect{u}_1,\ldots,\vect{u}_k$. Also, given any finite set of vectors,
we can find a subset of the vectors that is linearly independent and
has the same span.

We finish this section with a useful observation about linear
independence. Namely, given a linearly independent set of vectors and
one more vector that is not in their span, then we can add the vector
to the set and it will remain linearly independent.

\begin{proposition}{Adding to a linearly independent set}{adding-linearly-independent}
  Suppose $\set{\vect{u}_1,\ldots,\vect{u}_k}$ is linearly
  independent and
  $\vect{v}\notin \sspan\set{\vect{u}_1,\ldots,\vect{u}_k}$. Then
  the set
  \begin{equation*}
    \set{\vect{u}_1,\ldots,\vect{u}_k,\vect{v}}
  \end{equation*}
  is also linearly independent.
\end{proposition}

\begin{proof}
  Assume, on the contrary, that the set were linearly dependent. Then
  by Proposition~\ref{prop:linear-dependence-redundant}, one of the
  vectors can be written as a linear combination of earlier vectors.
  This vector cannot be one of the $\vect{u}_i$, because
  $\vect{u}_1,\ldots,\vect{u}_k$ are linearly independent.  It
  also cannot be $\vect{v}$, because
  $\vect{v}\notin
  \sspan\set{\vect{u}_1,\ldots,\vect{u}_k}$. Therefore, our
  assumption cannot be true, and the set is linearly independent.
\end{proof}
